{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wconfid</th>\n",
       "      <th>pctid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.004</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.973</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wconfid  pctid      x      y      z\n",
       "0        1     20  1.004  0.090 -0.125\n",
       "1        1     20  1.004 -0.043 -0.125\n",
       "2        1     20  0.969  0.090 -0.121\n",
       "3        1     20  0.973 -0.012 -0.137\n",
       "4        1     20  1.000 -0.016 -0.121"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_red=pd.read_csv(\"Data/红酒白酒分类+回归（都可以）/winequality-red.csv\",sep=\";\",index_col=False)\n",
    "# dataset_white=pd.read_csv(\"Data/红酒白酒分类+回归（都可以）/winequality-white.csv\",sep=\";\",index_col=False)\n",
    "dataset = pd.read_csv(\"Data/加速度计 regression/accelerometer.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15314\n",
       "1    15303\n",
       "2    15283\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(dataset[\"wconfid\"])\n",
    "X = dataset.drop(\"wconfid\", axis=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=3)\n",
    "pd.value_counts(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45900, 4)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalization\n",
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer()\n",
    "Xn_train = norm.fit_transform(X_train)\n",
    "Xn_test = norm.transform(X_test)\n",
    "Xn_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 256)               1280      \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 199,427\n",
      "Trainable params: 199,427\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#NN model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(256, input_dim=4, activation=\"linear\"))\n",
    "model.add(keras.layers.Dense(256, activation=\"linear\"))\n",
    "model.add(keras.layers.Dense(256, activation=\"linear\"))\n",
    "model.add(keras.layers.Dense(256, activation=\"linear\"))\n",
    "model.add(keras.layers.Dense(3, activation=\"softmax\"))\n",
    "model.summary()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 107100 samples, validate on 45900 samples\n",
      "Epoch 1/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0997 - acc: 0.3326 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 2/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3326 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 3/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0989 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 4/1000\n",
      "107100/107100 [==============================] - 11s 101us/sample - loss: 1.0991 - acc: 0.3335 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 5/1000\n",
      "107100/107100 [==============================] - 11s 105us/sample - loss: 1.0989 - acc: 0.3336 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 6/1000\n",
      "107100/107100 [==============================] - 11s 105us/sample - loss: 1.0990 - acc: 0.3351 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 7/1000\n",
      "107100/107100 [==============================] - 12s 107us/sample - loss: 1.0988 - acc: 0.3363 - val_loss: 1.0989 - val_acc: 0.3330\n",
      "Epoch 8/1000\n",
      "107100/107100 [==============================] - 11s 104us/sample - loss: 1.0988 - acc: 0.3326 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 9/1000\n",
      "107100/107100 [==============================] - 11s 104us/sample - loss: 1.0989 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 10/1000\n",
      "107100/107100 [==============================] - 11s 106us/sample - loss: 1.0989 - acc: 0.3311 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 11/1000\n",
      "107100/107100 [==============================] - 12s 108us/sample - loss: 1.0989 - acc: 0.3340 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 12/1000\n",
      "107100/107100 [==============================] - 11s 107us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 13/1000\n",
      "107100/107100 [==============================] - 12s 109us/sample - loss: 1.0988 - acc: 0.3320 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 14/1000\n",
      "107100/107100 [==============================] - 12s 110us/sample - loss: 1.0989 - acc: 0.3317 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 15/1000\n",
      "107100/107100 [==============================] - 12s 109us/sample - loss: 1.0988 - acc: 0.3329 - val_loss: 1.0987 - val_acc: 0.3323\n",
      "Epoch 16/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0991 - acc: 0.3316 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 17/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3336 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 18/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0989 - acc: 0.3316 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 19/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3337 - val_loss: 1.0989 - val_acc: 0.3334\n",
      "Epoch 20/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3336 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 21/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0990 - acc: 0.3326 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 22/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0988 - acc: 0.3312 - val_loss: 1.0986 - val_acc: 0.3329\n",
      "Epoch 23/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3344 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 24/1000\n",
      "107100/107100 [==============================] - 10s 92us/sample - loss: 1.0989 - acc: 0.3349 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 25/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3341 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 26/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3315 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 27/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 28/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0989 - acc: 0.3340 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 29/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 30/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0990 - acc: 0.3335 - val_loss: 1.0990 - val_acc: 0.3334\n",
      "Epoch 31/1000\n",
      "107100/107100 [==============================] - 11s 104us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 32/1000\n",
      "107100/107100 [==============================] - 10s 92us/sample - loss: 1.0990 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3557\n",
      "Epoch 33/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3348 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 34/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0988 - acc: 0.3363 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 35/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3307 - val_loss: 1.0988 - val_acc: 0.3301\n",
      "Epoch 36/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0988 - acc: 0.3300 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 37/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0990 - acc: 0.3380 - val_loss: 1.0997 - val_acc: 0.3334\n",
      "Epoch 38/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3325 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 39/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3354 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 40/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3322 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 41/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3332 - val_loss: 1.0989 - val_acc: 0.3355\n",
      "Epoch 42/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0991 - acc: 0.3314 - val_loss: 1.0987 - val_acc: 0.3272\n",
      "Epoch 43/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0987 - acc: 0.3312 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 44/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3303 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 45/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3320 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 46/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3315 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 47/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3331 - val_loss: 1.0986 - val_acc: 0.3310\n",
      "Epoch 48/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0988 - acc: 0.3332 - val_loss: 1.0986 - val_acc: 0.3335\n",
      "Epoch 49/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3346 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 50/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3330 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 51/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3326 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 52/1000\n",
      "107100/107100 [==============================] - 11s 107us/sample - loss: 1.0989 - acc: 0.3337 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 53/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3337 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 54/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 55/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3351 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 56/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0991 - acc: 0.3322 - val_loss: 1.0986 - val_acc: 0.3892\n",
      "Epoch 57/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0989 - acc: 0.3351 - val_loss: 1.0989 - val_acc: 0.3336\n",
      "Epoch 58/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 59/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0991 - acc: 0.3357 - val_loss: 1.0986 - val_acc: 0.2956\n",
      "Epoch 60/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3342 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 61/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3334 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 62/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3331 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 63/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0989 - acc: 0.3346 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 64/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3314 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 65/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0990 - acc: 0.3346 - val_loss: 1.0986 - val_acc: 0.3355\n",
      "Epoch 66/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3336 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 67/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0990 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 68/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3335 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 69/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 70/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 71/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3337 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 72/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3317 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 73/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0991 - acc: 0.3334 - val_loss: 1.0986 - val_acc: 0.3418\n",
      "Epoch 74/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3295 - val_loss: 1.0986 - val_acc: 0.3335\n",
      "Epoch 75/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 76/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0989 - acc: 0.3316 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 77/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 78/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0990 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3017\n",
      "Epoch 79/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0988 - acc: 0.3308 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 80/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3305 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 81/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0989 - acc: 0.3359 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 82/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0988 - acc: 0.3301 - val_loss: 1.0992 - val_acc: 0.3334\n",
      "Epoch 83/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3347 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 84/1000\n",
      "107100/107100 [==============================] - 11s 103us/sample - loss: 1.0990 - acc: 0.3349 - val_loss: 1.0986 - val_acc: 0.3339\n",
      "Epoch 85/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3325 - val_loss: 1.0989 - val_acc: 0.3330\n",
      "Epoch 86/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3315 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 87/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0990 - acc: 0.3322 - val_loss: 1.0990 - val_acc: 0.3334\n",
      "Epoch 88/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 89/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3351 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 90/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3366 - val_loss: 1.0986 - val_acc: 0.3332\n",
      "Epoch 91/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0988 - acc: 0.3346 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 92/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0992 - acc: 0.3318 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 93/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0987 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 94/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3337 - val_loss: 1.1003 - val_acc: 0.3330\n",
      "Epoch 95/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 96/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0988 - acc: 0.3317 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 97/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0991 - acc: 0.3314 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 98/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0988 - acc: 0.3323 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 99/1000\n",
      "107100/107100 [==============================] - 11s 101us/sample - loss: 1.0988 - acc: 0.3337 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 100/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3334 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 101/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3355 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 102/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0990 - acc: 0.3310 - val_loss: 1.0986 - val_acc: 0.3277\n",
      "Epoch 103/1000\n",
      "107100/107100 [==============================] - 12s 109us/sample - loss: 1.0988 - acc: 0.3348 - val_loss: 1.0990 - val_acc: 0.3513\n",
      "Epoch 104/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0988 - acc: 0.3341 - val_loss: 1.0994 - val_acc: 0.3334\n",
      "Epoch 105/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 106/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3335 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 107/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3334\n",
      "Epoch 108/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3358 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 109/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0991 - acc: 0.3319 - val_loss: 1.0986 - val_acc: 0.3819\n",
      "Epoch 110/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3315 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 111/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3304 - val_loss: 1.0986 - val_acc: 0.3423\n",
      "Epoch 112/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3321 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 113/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0990 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 114/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 115/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 116/1000\n",
      "107100/107100 [==============================] - 11s 100us/sample - loss: 1.0989 - acc: 0.3343 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 117/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0987 - acc: 0.3358 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 118/1000\n",
      "107100/107100 [==============================] - 11s 102us/sample - loss: 1.0989 - acc: 0.3323 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 119/1000\n",
      "107100/107100 [==============================] - 11s 103us/sample - loss: 1.0989 - acc: 0.3334 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 120/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0989 - acc: 0.3309 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 121/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3335 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 122/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0987 - val_acc: 0.3085\n",
      "Epoch 123/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0990 - acc: 0.3336 - val_loss: 1.0986 - val_acc: 0.3231\n",
      "Epoch 124/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0989 - acc: 0.3326 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 125/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0989 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 126/1000\n",
      "107100/107100 [==============================] - 10s 92us/sample - loss: 1.0988 - acc: 0.3324 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 127/1000\n",
      "107100/107100 [==============================] - 11s 104us/sample - loss: 1.0989 - acc: 0.3345 - val_loss: 1.0990 - val_acc: 0.3354\n",
      "Epoch 128/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0990 - acc: 0.3343 - val_loss: 1.0989 - val_acc: 0.3336\n",
      "Epoch 129/1000\n",
      "107100/107100 [==============================] - 13s 117us/sample - loss: 1.0988 - acc: 0.3331 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 130/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 131/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0991 - acc: 0.3344 - val_loss: 1.0988 - val_acc: 0.3339\n",
      "Epoch 132/1000\n",
      "107100/107100 [==============================] - 10s 92us/sample - loss: 1.0988 - acc: 0.3308 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 133/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0989 - acc: 0.3321 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 134/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0992 - acc: 0.3321 - val_loss: 1.0986 - val_acc: 0.3332\n",
      "Epoch 135/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0988 - acc: 0.3317 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 136/1000\n",
      "107100/107100 [==============================] - 11s 103us/sample - loss: 1.0989 - acc: 0.3311 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 137/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0988 - acc: 0.3317 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 138/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0989 - acc: 0.3312 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 139/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0991 - acc: 0.3340 - val_loss: 1.0987 - val_acc: 0.3546\n",
      "Epoch 140/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3331 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 141/1000\n",
      "107100/107100 [==============================] - 12s 117us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 142/1000\n",
      "107100/107100 [==============================] - 11s 105us/sample - loss: 1.0990 - acc: 0.3332 - val_loss: 1.0991 - val_acc: 0.3330\n",
      "Epoch 143/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3314 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 144/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0990 - acc: 0.3338 - val_loss: 1.0986 - val_acc: 0.3332\n",
      "Epoch 145/1000\n",
      "107100/107100 [==============================] - 18s 165us/sample - loss: 1.0990 - acc: 0.3344 - val_loss: 1.0986 - val_acc: 0.3339\n",
      "Epoch 146/1000\n",
      "107100/107100 [==============================] - 15s 137us/sample - loss: 1.0989 - acc: 0.3313 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 147/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0991 - acc: 0.3296 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 148/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0987 - acc: 0.3329 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 149/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0990 - acc: 0.3355 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 150/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0990 - acc: 0.3304 - val_loss: 1.0986 - val_acc: 0.3277\n",
      "Epoch 151/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3335 - val_loss: 1.0988 - val_acc: 0.3319\n",
      "Epoch 152/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0990 - acc: 0.3314 - val_loss: 1.0987 - val_acc: 0.3313\n",
      "Epoch 153/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0987 - acc: 0.3329 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 154/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0990 - acc: 0.3318 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 155/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3319 - val_loss: 1.0986 - val_acc: 0.3708\n",
      "Epoch 156/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0990 - acc: 0.3327 - val_loss: 1.0995 - val_acc: 0.3334\n",
      "Epoch 157/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 158/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3316 - val_loss: 1.0987 - val_acc: 0.3316\n",
      "Epoch 159/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0990 - acc: 0.3323 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 160/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3327 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 161/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3317 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 162/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3325 - val_loss: 1.0998 - val_acc: 0.3336\n",
      "Epoch 163/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3304 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 164/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3320 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 165/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3341 - val_loss: 1.0986 - val_acc: 0.3350\n",
      "Epoch 166/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0990 - acc: 0.3340 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 167/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3335 - val_loss: 1.0989 - val_acc: 0.3334\n",
      "Epoch 168/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0989 - acc: 0.3337 - val_loss: 1.0992 - val_acc: 0.3278\n",
      "Epoch 169/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3322 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 170/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0990 - acc: 0.3353 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 171/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3313 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 172/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0988 - acc: 0.3324 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 173/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0990 - acc: 0.3323 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 174/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0990 - acc: 0.3359 - val_loss: 1.0986 - val_acc: 0.3415\n",
      "Epoch 175/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0988 - acc: 0.3322 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 176/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0990 - acc: 0.3328 - val_loss: 1.0986 - val_acc: 0.3299\n",
      "Epoch 177/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3323 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 178/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0989 - acc: 0.3300 - val_loss: 1.0989 - val_acc: 0.3334\n",
      "Epoch 179/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3319 - val_loss: 1.0986 - val_acc: 0.2904\n",
      "Epoch 180/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3349\n",
      "Epoch 181/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0987 - val_acc: 0.3664\n",
      "Epoch 182/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0990 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3341\n",
      "Epoch 183/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3345 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 184/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3310 - val_loss: 1.0989 - val_acc: 0.3330\n",
      "Epoch 185/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0989 - acc: 0.3350 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 186/1000\n",
      "107100/107100 [==============================] - 11s 101us/sample - loss: 1.0989 - acc: 0.3354 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 187/1000\n",
      "107100/107100 [==============================] - 12s 108us/sample - loss: 1.0989 - acc: 0.3353 - val_loss: 1.0986 - val_acc: 0.3214\n",
      "Epoch 188/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0988 - acc: 0.3354 - val_loss: 1.0987 - val_acc: 0.3342\n",
      "Epoch 189/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0990 - acc: 0.3342 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 190/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0988 - acc: 0.3328 - val_loss: 1.0986 - val_acc: 0.3183\n",
      "Epoch 191/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0989 - acc: 0.3343 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 192/1000\n",
      "107100/107100 [==============================] - 10s 92us/sample - loss: 1.0989 - acc: 0.3340 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 193/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0989 - acc: 0.3340 - val_loss: 1.0987 - val_acc: 0.3312\n",
      "Epoch 194/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0990 - acc: 0.3344 - val_loss: 1.0987 - val_acc: 0.3351\n",
      "Epoch 195/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 196/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0990 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3331\n",
      "Epoch 197/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0989 - acc: 0.3325 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 198/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3325 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 199/1000\n",
      "107100/107100 [==============================] - 10s 98us/sample - loss: 1.0989 - acc: 0.3323 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 200/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0989 - acc: 0.3349 - val_loss: 1.0986 - val_acc: 0.3339\n",
      "Epoch 201/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3308 - val_loss: 1.0987 - val_acc: 0.3283\n",
      "Epoch 202/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3330\n",
      "Epoch 203/1000\n",
      "107100/107100 [==============================] - 11s 102us/sample - loss: 1.0989 - acc: 0.3368 - val_loss: 1.0988 - val_acc: 0.3225\n",
      "Epoch 204/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0990 - acc: 0.3329 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 205/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0988 - acc: 0.3337 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 206/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0990 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 207/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0988 - acc: 0.3351 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 208/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0989 - acc: 0.3301 - val_loss: 1.0986 - val_acc: 0.3385\n",
      "Epoch 209/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0989 - acc: 0.3343 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 210/1000\n",
      "107100/107100 [==============================] - 11s 106us/sample - loss: 1.0988 - acc: 0.3322 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 211/1000\n",
      "107100/107100 [==============================] - 13s 125us/sample - loss: 1.0989 - acc: 0.3319 - val_loss: 1.1262 - val_acc: 0.3286\n",
      "Epoch 212/1000\n",
      "107100/107100 [==============================] - 13s 117us/sample - loss: 1.0989 - acc: 0.3347 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 213/1000\n",
      "107100/107100 [==============================] - 11s 106us/sample - loss: 1.0988 - acc: 0.3318 - val_loss: 1.0990 - val_acc: 0.3334\n",
      "Epoch 214/1000\n",
      "107100/107100 [==============================] - 12s 114us/sample - loss: 1.0989 - acc: 0.3323 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 215/1000\n",
      "107100/107100 [==============================] - 11s 100us/sample - loss: 1.0990 - acc: 0.3337 - val_loss: 1.0993 - val_acc: 0.3355\n",
      "Epoch 216/1000\n",
      "107100/107100 [==============================] - 10s 91us/sample - loss: 1.0988 - acc: 0.3328 - val_loss: 1.0988 - val_acc: 0.3283\n",
      "Epoch 217/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0990 - acc: 0.3334 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 218/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0988 - acc: 0.3321 - val_loss: 1.1000 - val_acc: 0.3334\n",
      "Epoch 219/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0990 - acc: 0.3348 - val_loss: 1.0987 - val_acc: 0.3426\n",
      "Epoch 220/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3175\n",
      "Epoch 221/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0988 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 222/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 223/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0988 - acc: 0.3315 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 224/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3340 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 225/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3331 - val_loss: 1.0986 - val_acc: 0.4050\n",
      "Epoch 226/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3310 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 227/1000\n",
      "107100/107100 [==============================] - 9s 84us/sample - loss: 1.0988 - acc: 0.3340 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 228/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0990 - acc: 0.3325 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 229/1000\n",
      "107100/107100 [==============================] - 9s 84us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 230/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3339 - val_loss: 1.0987 - val_acc: 0.3259\n",
      "Epoch 231/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3353 - val_loss: 1.0987 - val_acc: 0.3245\n",
      "Epoch 232/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3344 - val_loss: 1.0996 - val_acc: 0.3008\n",
      "Epoch 233/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3313 - val_loss: 1.0988 - val_acc: 0.3383\n",
      "Epoch 234/1000\n",
      "107100/107100 [==============================] - 9s 84us/sample - loss: 1.0988 - acc: 0.3327 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 235/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3306 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 236/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3336 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 237/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3336 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 238/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3327 - val_loss: 1.0986 - val_acc: 0.3324\n",
      "Epoch 239/1000\n",
      "107100/107100 [==============================] - 9s 84us/sample - loss: 1.0988 - acc: 0.3345 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 240/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3339 - val_loss: 1.0987 - val_acc: 0.3274\n",
      "Epoch 241/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0987 - acc: 0.3342 - val_loss: 1.0987 - val_acc: 0.3601\n",
      "Epoch 242/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0988 - acc: 0.3356 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 243/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0990 - acc: 0.3320 - val_loss: 1.0987 - val_acc: 0.3328\n",
      "Epoch 244/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3332 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 245/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3363\n",
      "Epoch 246/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3354 - val_loss: 1.0992 - val_acc: 0.3330\n",
      "Epoch 247/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3315 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 248/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0990 - acc: 0.3313 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 249/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0988 - acc: 0.3340 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 250/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0990 - acc: 0.3316 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 251/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3312 - val_loss: 1.0986 - val_acc: 0.3457\n",
      "Epoch 252/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3349 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 253/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 254/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3325 - val_loss: 1.0990 - val_acc: 0.3330\n",
      "Epoch 255/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3346 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 256/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0993 - acc: 0.3356 - val_loss: 1.0986 - val_acc: 0.3297\n",
      "Epoch 257/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 258/1000\n",
      "107100/107100 [==============================] - 9s 84us/sample - loss: 1.0988 - acc: 0.3343 - val_loss: 1.0987 - val_acc: 0.3682\n",
      "Epoch 259/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3342 - val_loss: 1.0987 - val_acc: 0.3385\n",
      "Epoch 260/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0992 - acc: 0.3319 - val_loss: 1.0987 - val_acc: 0.3259\n",
      "Epoch 261/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0987 - acc: 0.3320 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 262/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3359 - val_loss: 1.0987 - val_acc: 0.3344\n",
      "Epoch 263/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3294 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 264/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 265/1000\n",
      "107100/107100 [==============================] - 9s 84us/sample - loss: 1.0989 - acc: 0.3338 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 266/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3321 - val_loss: 1.0990 - val_acc: 0.3330\n",
      "Epoch 267/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0988 - acc: 0.3345 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 268/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0988 - acc: 0.3339 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 269/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0990 - acc: 0.3334 - val_loss: 1.0990 - val_acc: 0.3334\n",
      "Epoch 270/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3336 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 271/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0988 - acc: 0.3352 - val_loss: 1.0986 - val_acc: 0.3249\n",
      "Epoch 272/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3309 - val_loss: 1.0986 - val_acc: 0.3353\n",
      "Epoch 273/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0990 - acc: 0.3362 - val_loss: 1.0992 - val_acc: 0.3349\n",
      "Epoch 274/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0989 - acc: 0.3296 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 275/1000\n",
      "107100/107100 [==============================] - 10s 91us/sample - loss: 1.0987 - acc: 0.3338 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 276/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0989 - acc: 0.3335 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 277/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3337 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 278/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3320 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 279/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3349 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 280/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3315 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 281/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3327 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 282/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3326 - val_loss: 1.1029 - val_acc: 0.3328\n",
      "Epoch 283/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3358\n",
      "Epoch 284/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3331 - val_loss: 1.0986 - val_acc: 0.3412\n",
      "Epoch 285/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3315 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 286/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0988 - acc: 0.3344 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 287/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0991 - acc: 0.3322 - val_loss: 1.0986 - val_acc: 0.3325\n",
      "Epoch 288/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 289/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3366 - val_loss: 1.0987 - val_acc: 0.3273\n",
      "Epoch 290/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3321 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 291/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0989 - acc: 0.3338 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 292/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3321 - val_loss: 1.0986 - val_acc: 0.3381\n",
      "Epoch 293/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3317 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 294/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 295/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3331 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 296/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3304 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 297/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0987 - acc: 0.3316 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 298/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0989 - acc: 0.3308 - val_loss: 1.0988 - val_acc: 0.3634\n",
      "Epoch 299/1000\n",
      "107100/107100 [==============================] - 10s 91us/sample - loss: 1.0989 - acc: 0.3338 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 300/1000\n",
      "107100/107100 [==============================] - 10s 91us/sample - loss: 1.0990 - acc: 0.3346 - val_loss: 1.0988 - val_acc: 0.3338\n",
      "Epoch 301/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0988 - acc: 0.3320 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 302/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3339 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 303/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3318 - val_loss: 1.0992 - val_acc: 0.3326\n",
      "Epoch 304/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3334 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 305/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0989 - acc: 0.3342 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 306/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3716\n",
      "Epoch 307/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3325 - val_loss: 1.0988 - val_acc: 0.3341\n",
      "Epoch 308/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0988 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 309/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3351 - val_loss: 1.0991 - val_acc: 0.3337\n",
      "Epoch 310/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0989 - acc: 0.3328 - val_loss: 1.0986 - val_acc: 0.3771\n",
      "Epoch 311/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3331 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 312/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0991 - acc: 0.3300 - val_loss: 1.0986 - val_acc: 0.3383\n",
      "Epoch 313/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0988 - acc: 0.3313 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 314/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3338 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 315/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0990 - acc: 0.3311 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 316/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0987 - acc: 0.3350 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 317/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0990 - acc: 0.3327 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 318/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0988 - acc: 0.3336 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 319/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3324 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 320/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0989 - acc: 0.3322 - val_loss: 1.0986 - val_acc: 0.3201\n",
      "Epoch 321/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3358 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 322/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0988 - acc: 0.3343 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 323/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0990 - acc: 0.3329 - val_loss: 1.0986 - val_acc: 0.3339\n",
      "Epoch 324/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0989 - acc: 0.3315 - val_loss: 1.0986 - val_acc: 0.3631\n",
      "Epoch 325/1000\n",
      "107100/107100 [==============================] - 12s 115us/sample - loss: 1.0989 - acc: 0.3335 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 326/1000\n",
      "107100/107100 [==============================] - 11s 105us/sample - loss: 1.0991 - acc: 0.3338 - val_loss: 1.0989 - val_acc: 0.3330\n",
      "Epoch 327/1000\n",
      "107100/107100 [==============================] - 11s 103us/sample - loss: 1.0988 - acc: 0.3324 - val_loss: 1.0999 - val_acc: 0.3299\n",
      "Epoch 328/1000\n",
      "107100/107100 [==============================] - 11s 105us/sample - loss: 1.0989 - acc: 0.3378 - val_loss: 1.0986 - val_acc: 0.3644\n",
      "Epoch 329/1000\n",
      "107100/107100 [==============================] - 13s 121us/sample - loss: 1.0987 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 330/1000\n",
      "107100/107100 [==============================] - 12s 110us/sample - loss: 1.0989 - acc: 0.3312 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 331/1000\n",
      "107100/107100 [==============================] - 13s 118us/sample - loss: 1.0990 - acc: 0.3342 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 332/1000\n",
      "107100/107100 [==============================] - 12s 112us/sample - loss: 1.0989 - acc: 0.3312 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 333/1000\n",
      "107100/107100 [==============================] - 11s 100us/sample - loss: 1.0989 - acc: 0.3324 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 334/1000\n",
      "107100/107100 [==============================] - 12s 110us/sample - loss: 1.0989 - acc: 0.3331 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 335/1000\n",
      "107100/107100 [==============================] - 11s 100us/sample - loss: 1.0988 - acc: 0.3334 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 336/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3328 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 337/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0989 - acc: 0.3342 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 338/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 339/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0988 - acc: 0.3363 - val_loss: 1.0992 - val_acc: 0.3334\n",
      "Epoch 340/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0989 - acc: 0.3349 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 341/1000\n",
      "107100/107100 [==============================] - 10s 91us/sample - loss: 1.0989 - acc: 0.3337 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 342/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0988 - acc: 0.3341 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 343/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3320 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 344/1000\n",
      "107100/107100 [==============================] - 11s 103us/sample - loss: 1.0990 - acc: 0.3314 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 345/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0989 - acc: 0.3326 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 346/1000\n",
      "107100/107100 [==============================] - 11s 100us/sample - loss: 1.0990 - acc: 0.3304 - val_loss: 1.0995 - val_acc: 0.3330\n",
      "Epoch 347/1000\n",
      "107100/107100 [==============================] - 11s 104us/sample - loss: 1.0988 - acc: 0.3326 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 348/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0989 - acc: 0.3343 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 349/1000\n",
      "107100/107100 [==============================] - 10s 92us/sample - loss: 1.0989 - acc: 0.3298 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 350/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0988 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3363\n",
      "Epoch 351/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0991 - acc: 0.3347 - val_loss: 1.0991 - val_acc: 0.3416\n",
      "Epoch 352/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0990 - acc: 0.3325 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 353/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0988 - acc: 0.3344 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 354/1000\n",
      "107100/107100 [==============================] - 10s 91us/sample - loss: 1.0988 - acc: 0.3323 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 355/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0990 - acc: 0.3348 - val_loss: 1.0986 - val_acc: 0.3359\n",
      "Epoch 356/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3349 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 357/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0991 - acc: 0.3355 - val_loss: 1.0986 - val_acc: 0.3352\n",
      "Epoch 358/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3345 - val_loss: 1.0988 - val_acc: 0.3334\n",
      "Epoch 359/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3325 - val_loss: 1.0986 - val_acc: 0.3286\n",
      "Epoch 360/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3346 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 361/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0990 - acc: 0.3316 - val_loss: 1.0992 - val_acc: 0.3334\n",
      "Epoch 362/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3356 - val_loss: 1.1001 - val_acc: 0.3283\n",
      "Epoch 363/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3358 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 364/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3336 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 365/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0990 - acc: 0.3348 - val_loss: 1.0991 - val_acc: 0.3330\n",
      "Epoch 366/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0992 - acc: 0.3344 - val_loss: 1.0994 - val_acc: 0.3336\n",
      "Epoch 367/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3331 - val_loss: 1.0988 - val_acc: 0.3371\n",
      "Epoch 368/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3339 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 369/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3337 - val_loss: 1.0986 - val_acc: 0.3371\n",
      "Epoch 370/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0990 - acc: 0.3329 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 371/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3345 - val_loss: 1.0986 - val_acc: 0.3349\n",
      "Epoch 372/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3327 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 373/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3315 - val_loss: 1.0986 - val_acc: 0.3310\n",
      "Epoch 374/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0989 - acc: 0.3334 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 375/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3326 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 376/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3344 - val_loss: 1.1013 - val_acc: 0.3316\n",
      "Epoch 377/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 378/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0989 - acc: 0.3352 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 379/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3291 - val_loss: 1.0987 - val_acc: 0.3353\n",
      "Epoch 380/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0991 - acc: 0.3342 - val_loss: 1.0990 - val_acc: 0.3509\n",
      "Epoch 381/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3361 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 382/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3340 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 383/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3349 - val_loss: 1.0987 - val_acc: 0.3338\n",
      "Epoch 384/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3356 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 385/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3340 - val_loss: 1.1085 - val_acc: 0.3336\n",
      "Epoch 386/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3298 - val_loss: 1.0986 - val_acc: 0.3345\n",
      "Epoch 387/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3319 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 388/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3331 - val_loss: 1.0986 - val_acc: 0.3846\n",
      "Epoch 389/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3353 - val_loss: 1.0989 - val_acc: 0.3364\n",
      "Epoch 390/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3347 - val_loss: 1.0986 - val_acc: 0.3211\n",
      "Epoch 391/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3315 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 392/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3330 - val_loss: 1.0986 - val_acc: 0.3297\n",
      "Epoch 393/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3304 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 394/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3335 - val_loss: 1.0987 - val_acc: 0.3197\n",
      "Epoch 395/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3327 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 396/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3308 - val_loss: 1.0990 - val_acc: 0.3380\n",
      "Epoch 397/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3362 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 398/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3329 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 399/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3352 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 400/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0987 - acc: 0.3350 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 401/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0991 - acc: 0.3341 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 402/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3311 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 403/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3325 - val_loss: 1.0986 - val_acc: 0.3378\n",
      "Epoch 404/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3317 - val_loss: 1.0987 - val_acc: 0.3147\n",
      "Epoch 405/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3329 - val_loss: 1.0987 - val_acc: 0.3341\n",
      "Epoch 406/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0990 - acc: 0.3321 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 407/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3296 - val_loss: 1.0986 - val_acc: 0.3870\n",
      "Epoch 408/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3347 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 409/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3347 - val_loss: 1.0987 - val_acc: 0.3328\n",
      "Epoch 410/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0992 - acc: 0.3328 - val_loss: 1.0988 - val_acc: 0.3340\n",
      "Epoch 411/1000\n",
      "107100/107100 [==============================] - 10s 93us/sample - loss: 1.0988 - acc: 0.3357 - val_loss: 1.0987 - val_acc: 0.3462\n",
      "Epoch 412/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0989 - acc: 0.3338 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 413/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0989 - acc: 0.3313 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 414/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3320 - val_loss: 1.0998 - val_acc: 0.3336\n",
      "Epoch 415/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3327 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 416/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0986 - val_acc: 0.3337\n",
      "Epoch 417/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0988 - acc: 0.3330 - val_loss: 1.0987 - val_acc: 0.3299\n",
      "Epoch 418/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0987 - acc: 0.3329 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 419/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3360 - val_loss: 1.0987 - val_acc: 0.3373\n",
      "Epoch 420/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3336 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 421/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3338 - val_loss: 1.1070 - val_acc: 0.3330\n",
      "Epoch 422/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3304 - val_loss: 1.0987 - val_acc: 0.3349\n",
      "Epoch 423/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3332\n",
      "Epoch 424/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0990 - acc: 0.3326 - val_loss: 1.0987 - val_acc: 0.3331\n",
      "Epoch 425/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3325 - val_loss: 1.0989 - val_acc: 0.3337\n",
      "Epoch 426/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0987 - acc: 0.3362 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 427/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0990 - acc: 0.3319 - val_loss: 1.0986 - val_acc: 0.3325\n",
      "Epoch 428/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3328 - val_loss: 1.0986 - val_acc: 0.3332\n",
      "Epoch 429/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0992 - acc: 0.3357 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 430/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0989 - acc: 0.3344 - val_loss: 1.0991 - val_acc: 0.3330\n",
      "Epoch 431/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0988 - acc: 0.3347 - val_loss: 1.1019 - val_acc: 0.3365\n",
      "Epoch 432/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3326 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 433/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3323 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 434/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0988 - acc: 0.3352 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 435/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3320 - val_loss: 1.0989 - val_acc: 0.3336\n",
      "Epoch 436/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3315 - val_loss: 1.0986 - val_acc: 0.3442\n",
      "Epoch 437/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0988 - acc: 0.3322 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 438/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0991 - acc: 0.3317 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 439/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3331 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 440/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0990 - acc: 0.3352 - val_loss: 1.0988 - val_acc: 0.3526\n",
      "Epoch 441/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3343 - val_loss: 1.0986 - val_acc: 0.3393\n",
      "Epoch 442/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3299 - val_loss: 1.1005 - val_acc: 0.3334\n",
      "Epoch 443/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3343 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 444/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3303 - val_loss: 1.0986 - val_acc: 0.3331\n",
      "Epoch 445/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3298 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 446/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0989 - acc: 0.3327 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 447/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 448/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0988 - acc: 0.3337 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 449/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3311 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 450/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3317 - val_loss: 1.0987 - val_acc: 0.3352\n",
      "Epoch 451/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3335 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 452/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3308 - val_loss: 1.0986 - val_acc: 0.3336\n",
      "Epoch 453/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0988 - acc: 0.3327 - val_loss: 1.0987 - val_acc: 0.2798\n",
      "Epoch 454/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0991 - acc: 0.3335 - val_loss: 1.0986 - val_acc: 0.3295\n",
      "Epoch 455/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0987 - acc: 0.3355 - val_loss: 1.0990 - val_acc: 0.3334\n",
      "Epoch 456/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3348 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 457/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3344 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 458/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3358 - val_loss: 1.0987 - val_acc: 0.3271\n",
      "Epoch 459/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0991 - acc: 0.3306 - val_loss: 1.0986 - val_acc: 0.3431\n",
      "Epoch 460/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3329 - val_loss: 1.0989 - val_acc: 0.3330\n",
      "Epoch 461/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3359 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 462/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3317 - val_loss: 1.0989 - val_acc: 0.3330\n",
      "Epoch 463/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3332 - val_loss: 1.0986 - val_acc: 0.3356\n",
      "Epoch 464/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3311 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 465/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0988 - acc: 0.3345 - val_loss: 1.0986 - val_acc: 0.3366\n",
      "Epoch 466/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0991 - acc: 0.3350 - val_loss: 1.0987 - val_acc: 0.3386\n",
      "Epoch 467/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3329 - val_loss: 1.0993 - val_acc: 0.3269\n",
      "Epoch 468/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 469/1000\n",
      "107100/107100 [==============================] - 10s 92us/sample - loss: 1.0989 - acc: 0.3330 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 470/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0990 - acc: 0.3320 - val_loss: 1.0986 - val_acc: 0.3334\n",
      "Epoch 471/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0991 - acc: 0.3314 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 472/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0988 - acc: 0.3343 - val_loss: 1.0986 - val_acc: 0.3971\n",
      "Epoch 473/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3348 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 474/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3353 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 475/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0988 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3336\n",
      "Epoch 476/1000\n",
      "107100/107100 [==============================] - 9s 84us/sample - loss: 1.0988 - acc: 0.3326 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 477/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3346 - val_loss: 1.0986 - val_acc: 0.3332\n",
      "Epoch 478/1000\n",
      "107100/107100 [==============================] - 10s 89us/sample - loss: 1.0988 - acc: 0.3335 - val_loss: 1.0987 - val_acc: 0.3378\n",
      "Epoch 479/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0989 - acc: 0.3345 - val_loss: 1.0987 - val_acc: 0.3346\n",
      "Epoch 480/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0989 - acc: 0.3364 - val_loss: 1.0987 - val_acc: 0.3393\n",
      "Epoch 481/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3336 - val_loss: 1.0987 - val_acc: 0.3264\n",
      "Epoch 482/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3315 - val_loss: 1.1010 - val_acc: 0.3334\n",
      "Epoch 483/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3318 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 484/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0988 - acc: 0.3329 - val_loss: 1.0986 - val_acc: 0.3378\n",
      "Epoch 485/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3332 - val_loss: 1.0990 - val_acc: 0.3334\n",
      "Epoch 486/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 487/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0988 - acc: 0.3362 - val_loss: 1.0993 - val_acc: 0.3330\n",
      "Epoch 488/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0991 - acc: 0.3317 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 489/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0988 - acc: 0.3342 - val_loss: 1.0986 - val_acc: 0.3266\n",
      "Epoch 490/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3319 - val_loss: 1.0986 - val_acc: 0.3069\n",
      "Epoch 491/1000\n",
      "107100/107100 [==============================] - 9s 86us/sample - loss: 1.0991 - acc: 0.3367 - val_loss: 1.0987 - val_acc: 0.3350\n",
      "Epoch 492/1000\n",
      "107100/107100 [==============================] - 9s 89us/sample - loss: 1.0989 - acc: 0.3344 - val_loss: 1.0989 - val_acc: 0.3330\n",
      "Epoch 493/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3325 - val_loss: 1.0988 - val_acc: 0.3336\n",
      "Epoch 494/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0988 - acc: 0.3328 - val_loss: 1.0988 - val_acc: 0.3330\n",
      "Epoch 495/1000\n",
      "107100/107100 [==============================] - 10s 90us/sample - loss: 1.0988 - acc: 0.3312 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 496/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0990 - acc: 0.3321 - val_loss: 1.0987 - val_acc: 0.3330\n",
      "Epoch 497/1000\n",
      "107100/107100 [==============================] - 9s 88us/sample - loss: 1.0989 - acc: 0.3357 - val_loss: 1.0986 - val_acc: 0.3330\n",
      "Epoch 498/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3355\n",
      "Epoch 499/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0991 - acc: 0.3304 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 500/1000\n",
      "107100/107100 [==============================] - 9s 87us/sample - loss: 1.0989 - acc: 0.3339 - val_loss: 1.0987 - val_acc: 0.3571\n",
      "Epoch 501/1000\n",
      "107100/107100 [==============================] - 9s 85us/sample - loss: 1.0988 - acc: 0.3319 - val_loss: 1.0986 - val_acc: 0.3344\n",
      "Epoch 502/1000\n",
      "107100/107100 [==============================] - 10s 91us/sample - loss: 1.0989 - acc: 0.3338 - val_loss: 1.0986 - val_acc: 0.3337\n",
      "Epoch 503/1000\n",
      "107100/107100 [==============================] - 12s 111us/sample - loss: 1.0988 - acc: 0.3349 - val_loss: 1.0987 - val_acc: 0.3334\n",
      "Epoch 504/1000\n",
      "107100/107100 [==============================] - 11s 99us/sample - loss: 1.0990 - acc: 0.3334 - val_loss: 1.0986 - val_acc: 0.3392\n",
      "Epoch 505/1000\n",
      "107100/107100 [==============================] - 11s 98us/sample - loss: 1.0988 - acc: 0.3358 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 506/1000\n",
      "107100/107100 [==============================] - 11s 103us/sample - loss: 1.0989 - acc: 0.3331 - val_loss: 1.0986 - val_acc: 0.2890\n",
      "Epoch 507/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3353 - val_loss: 1.0986 - val_acc: 0.3620\n",
      "Epoch 508/1000\n",
      "107100/107100 [==============================] - 11s 104us/sample - loss: 1.0988 - acc: 0.3437 - val_loss: 1.0986 - val_acc: 0.3771\n",
      "Epoch 509/1000\n",
      "107100/107100 [==============================] - 12s 114us/sample - loss: 1.0989 - acc: 0.3434 - val_loss: 1.0989 - val_acc: 0.3276\n",
      "Epoch 510/1000\n",
      "107100/107100 [==============================] - 14s 127us/sample - loss: 1.0987 - acc: 0.3376 - val_loss: 1.0986 - val_acc: 0.3044\n",
      "Epoch 511/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0989 - acc: 0.3437 - val_loss: 1.0987 - val_acc: 0.2989\n",
      "Epoch 512/1000\n",
      "107100/107100 [==============================] - 11s 100us/sample - loss: 1.0987 - acc: 0.3422 - val_loss: 1.0985 - val_acc: 0.4023\n",
      "Epoch 513/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3411 - val_loss: 1.0986 - val_acc: 0.3739\n",
      "Epoch 514/1000\n",
      "107100/107100 [==============================] - 10s 95us/sample - loss: 1.0989 - acc: 0.3453 - val_loss: 1.0986 - val_acc: 0.3383\n",
      "Epoch 515/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3421 - val_loss: 1.0986 - val_acc: 0.2765\n",
      "Epoch 516/1000\n",
      "107100/107100 [==============================] - 10s 94us/sample - loss: 1.0988 - acc: 0.3509 - val_loss: 1.0986 - val_acc: 0.3251\n",
      "Epoch 517/1000\n",
      "107100/107100 [==============================] - 11s 100us/sample - loss: 1.0989 - acc: 0.3453 - val_loss: 1.0994 - val_acc: 0.3369\n",
      "Epoch 518/1000\n",
      "107100/107100 [==============================] - 11s 101us/sample - loss: 1.0987 - acc: 0.3475 - val_loss: 1.1033 - val_acc: 0.3347\n",
      "Epoch 519/1000\n",
      "107100/107100 [==============================] - 10s 96us/sample - loss: 1.0988 - acc: 0.3483 - val_loss: 1.0986 - val_acc: 0.3252\n",
      "Epoch 520/1000\n",
      "107100/107100 [==============================] - 10s 97us/sample - loss: 1.0988 - acc: 0.3486 - val_loss: 1.0986 - val_acc: 0.3804\n",
      "Epoch 521/1000\n",
      "107100/107100 [==============================] - 11s 100us/sample - loss: 1.0988 - acc: 0.3538 - val_loss: 1.1098 - val_acc: 0.3381\n",
      "Epoch 522/1000\n",
      "107100/107100 [==============================] - 11s 102us/sample - loss: 1.0989 - acc: 0.3451 - val_loss: 1.0987 - val_acc: 0.3243\n",
      "Epoch 523/1000\n",
      "107100/107100 [==============================] - 13s 123us/sample - loss: 1.0988 - acc: 0.3513 - val_loss: 1.0986 - val_acc: 0.3390\n",
      "Epoch 524/1000\n",
      " 14240/107100 [==>...........................] - ETA: 9s - loss: 1.0984 - acc: 0.3428"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_9716\\546713441.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#train the model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mXn_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1000\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mXn_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32md:\\Anaconda\\envs\\mle_tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001B[0m\n\u001B[0;32m    778\u001B[0m           \u001B[0mvalidation_steps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidation_steps\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    779\u001B[0m           \u001B[0mvalidation_freq\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidation_freq\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 780\u001B[1;33m           steps_name='steps_per_epoch')\n\u001B[0m\u001B[0;32m    781\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    782\u001B[0m   def evaluate(self,\n",
      "\u001B[1;32md:\\Anaconda\\envs\\mle_tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001B[0m in \u001B[0;36mmodel_iteration\u001B[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001B[0m\n\u001B[0;32m    361\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m         \u001B[1;31m# Get outputs.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 363\u001B[1;33m         \u001B[0mbatch_outs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mins_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    364\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_outs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    365\u001B[0m           \u001B[0mbatch_outs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mbatch_outs\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\Anaconda\\envs\\mle_tf\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   3290\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3291\u001B[0m     fetched = self._callable_fn(*array_vals,\n\u001B[1;32m-> 3292\u001B[1;33m                                 run_metadata=self.run_metadata)\n\u001B[0m\u001B[0;32m   3293\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_fetch_callbacks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfetched\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fetches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3294\u001B[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001B[1;32md:\\Anaconda\\envs\\mle_tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1456\u001B[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001B[0;32m   1457\u001B[0m                                                \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1458\u001B[1;33m                                                run_metadata_ptr)\n\u001B[0m\u001B[0;32m   1459\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1460\u001B[0m           \u001B[0mproto_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(np.array(Xn_train), np.array(y_train), epochs=1000, validation_data=(np.array(Xn_test), np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 1]\n",
      "[1 0 2 ... 0 1 0]\n",
      "[[ 2243 13071     0]\n",
      " [ 2032 13271     0]\n",
      " [    0 15283     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33799564270152505"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = model.predict_classes(Xn_test)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.reshape(y_test.shape[0], 1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 31.56%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(np.array(Xn_test), np.array(y_test), verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "Label                                                                 \n",
       "red              7.4              0.70         0.00             1.9   \n",
       "red              7.8              0.88         0.00             2.6   \n",
       "red              7.8              0.76         0.04             2.3   \n",
       "red             11.2              0.28         0.56             1.9   \n",
       "red              7.4              0.70         0.00             1.9   \n",
       "...              ...               ...          ...             ...   \n",
       "white            6.2              0.21         0.29             1.6   \n",
       "white            6.6              0.32         0.36             8.0   \n",
       "white            6.5              0.24         0.19             1.2   \n",
       "white            5.5              0.29         0.30             1.1   \n",
       "white            6.0              0.21         0.38             0.8   \n",
       "\n",
       "       chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "Label                                                                        \n",
       "red        0.076                 11.0                  34.0  0.99780  3.51   \n",
       "red        0.098                 25.0                  67.0  0.99680  3.20   \n",
       "red        0.092                 15.0                  54.0  0.99700  3.26   \n",
       "red        0.075                 17.0                  60.0  0.99800  3.16   \n",
       "red        0.076                 11.0                  34.0  0.99780  3.51   \n",
       "...          ...                  ...                   ...      ...   ...   \n",
       "white      0.039                 24.0                  92.0  0.99114  3.27   \n",
       "white      0.047                 57.0                 168.0  0.99490  3.15   \n",
       "white      0.041                 30.0                 111.0  0.99254  2.99   \n",
       "white      0.022                 20.0                 110.0  0.98869  3.34   \n",
       "white      0.020                 22.0                  98.0  0.98941  3.26   \n",
       "\n",
       "       sulphates  alcohol  quality  \n",
       "Label                               \n",
       "red         0.56      9.4        5  \n",
       "red         0.68      9.8        5  \n",
       "red         0.65      9.8        5  \n",
       "red         0.58      9.8        6  \n",
       "red         0.56      9.4        5  \n",
       "...          ...      ...      ...  \n",
       "white       0.50     11.2        6  \n",
       "white       0.46      9.6        5  \n",
       "white       0.46      9.4        6  \n",
       "white       0.38     12.8        7  \n",
       "white       0.32     11.8        6  \n",
       "\n",
       "[6497 rows x 12 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_red=pd.read_csv(\"Data/红酒白酒分类+回归（都可以）/winequality-red.csv\",sep=\";\",index_col=False)\n",
    "dataset_white=pd.read_csv(\"Data/红酒白酒分类+回归（都可以）/winequality-white.csv\",sep=\";\",index_col=False)\n",
    "# dataset_red.head(n=len(dataset_red))\n",
    "red=[\"red\"]*len(dataset_red)\n",
    "dataset_red.insert(0,\"Label\",red)\n",
    "white=[\"white\"]*len(dataset_white)\n",
    "dataset_white.insert(0,\"Label\",white)\n",
    "# dataset_white.head(n=5)\n",
    "dataset=pd.concat([dataset_red,dataset_white],axis=0)\n",
    "dataset.set_index(\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1457\n",
       "0     493\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "dataset[\"Label\"]=le.fit_transform(dataset[\"Label\"])\n",
    "y = dataset[\"Label\"]\n",
    "X = dataset.drop('Label',axis=1)\n",
    "\n",
    "#split the dataset into trainning set and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=2)\n",
    "# dataset['Label'].value_counts(normalize=True)\n",
    "X\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03929095, 0.00104005, 0.00173342, ..., 0.0030046 , 0.05200273,\n",
       "        0.04044657],\n",
       "       [0.03270101, 0.00206533, 0.00120477, ..., 0.00361432, 0.0573702 ,\n",
       "        0.03442212],\n",
       "       [0.03880164, 0.00211127, 0.00291012, ..., 0.00251069, 0.05021388,\n",
       "        0.02853061],\n",
       "       ...,\n",
       "       [0.0646795 , 0.00207898, 0.00358047, ..., 0.00669895, 0.1166541 ,\n",
       "        0.06929947],\n",
       "       [0.0538591 , 0.0023417 , 0.00351255, ..., 0.00366866, 0.08664289,\n",
       "        0.05463966],\n",
       "       [0.04715094, 0.00092873, 0.00214322, ..., 0.00321484, 0.07358404,\n",
       "        0.04286449]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalization\n",
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer()\n",
    "norm = norm.fit(X_train)\n",
    "Xn_train = norm.transform(X_train)\n",
    "Xn_test = norm.transform(X_test)\n",
    "Xn_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.92909538e-02, 1.04005466e-03, 9.88051926e-01, ...,\n",
       "        3.00460235e-03, 5.20027330e-02, 4.04465701e-02],\n",
       "       [3.27010135e-02, 2.06532717e-03, 9.52345305e-01, ...,\n",
       "        3.61432254e-03, 5.73701991e-02, 3.44221195e-02],\n",
       "       [3.88016352e-02, 2.11126544e-03, 9.30098020e-01, ...,\n",
       "        2.51069404e-03, 5.02138808e-02, 2.85306141e-02],\n",
       "       ...,\n",
       "       [6.46795021e-02, 2.07898400e-03, 9.70192532e-01, ...,\n",
       "        6.69894843e-03, 1.16654102e-01, 6.92994665e-02],\n",
       "       [5.38590961e-02, 2.34169983e-03, 9.52291265e-01, ...,\n",
       "        3.66866307e-03, 8.66428938e-02, 5.46396627e-02],\n",
       "       [4.71509383e-02, 9.28730604e-04, 9.35874685e-01, ...,\n",
       "        3.21483670e-03, 7.35840401e-02, 4.28644894e-02]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector = SelectKBest(chi2,k=8)\n",
    "Xnf_train = selector.fit_transform(Xn_train,y_train)\n",
    "Xnf_test = selector.transform(Xn_test)\n",
    "Xnf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 397,057\n",
      "Trainable params: 397,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#defining ANN model with two hidden layers, first hidden layer has 20 neurons, the next one has 10 neurons\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(256, input_dim=8,activation=\"linear\"))\n",
    "model.add(keras.layers.Dense(512, activation=\"linear\"))\n",
    "model.add(keras.layers.Dense(512, activation=\"linear\"))\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "# model.add(keras.layers.Dense(1, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4547 samples, validate on 1950 samples\n",
      "Epoch 1/500\n",
      "4547/4547 [==============================] - 1s 245us/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 2/500\n",
      "4547/4547 [==============================] - 1s 144us/sample - loss: 7.3210e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 3/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 7.3210e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 4/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 7.3210e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 5/500\n",
      "4547/4547 [==============================] - 1s 138us/sample - loss: 7.3209e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 6/500\n",
      "4547/4547 [==============================] - 1s 141us/sample - loss: 7.3208e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 7/500\n",
      "4547/4547 [==============================] - 1s 144us/sample - loss: 7.3208e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 8/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 7.3206e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 9/500\n",
      "4547/4547 [==============================] - 1s 137us/sample - loss: 7.3204e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 10/500\n",
      "4547/4547 [==============================] - 1s 138us/sample - loss: 7.3203e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 11/500\n",
      "4547/4547 [==============================] - 1s 137us/sample - loss: 7.3201e-11 - acc: 1.0000 - val_loss: 6.4776 - val_acc: 0.7472\n",
      "Epoch 12/500\n",
      "4547/4547 [==============================] - 1s 141us/sample - loss: 7.3198e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 13/500\n",
      "4547/4547 [==============================] - 1s 185us/sample - loss: 7.3195e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 14/500\n",
      "4547/4547 [==============================] - 1s 175us/sample - loss: 7.3192e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 15/500\n",
      "4547/4547 [==============================] - 1s 172us/sample - loss: 7.3189e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 16/500\n",
      "4547/4547 [==============================] - 1s 169us/sample - loss: 7.3185e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 17/500\n",
      "4547/4547 [==============================] - 1s 118us/sample - loss: 7.3182e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 18/500\n",
      "4547/4547 [==============================] - 1s 118us/sample - loss: 7.3177e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 19/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 7.3171e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 20/500\n",
      "4547/4547 [==============================] - 1s 124us/sample - loss: 7.3165e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 21/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 7.3158e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 22/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 7.3151e-11 - acc: 1.0000 - val_loss: 6.4777 - val_acc: 0.7472\n",
      "Epoch 23/500\n",
      "4547/4547 [==============================] - 1s 114us/sample - loss: 7.3144e-11 - acc: 1.0000 - val_loss: 6.4778 - val_acc: 0.7472\n",
      "Epoch 24/500\n",
      "4547/4547 [==============================] - 1s 117us/sample - loss: 7.3134e-11 - acc: 1.0000 - val_loss: 6.4778 - val_acc: 0.7472\n",
      "Epoch 25/500\n",
      "4547/4547 [==============================] - 1s 119us/sample - loss: 7.3125e-11 - acc: 1.0000 - val_loss: 6.4778 - val_acc: 0.7472\n",
      "Epoch 26/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 7.3109e-11 - acc: 1.0000 - val_loss: 6.4778 - val_acc: 0.7472\n",
      "Epoch 27/500\n",
      "4547/4547 [==============================] - 1s 119us/sample - loss: 7.3098e-11 - acc: 1.0000 - val_loss: 6.4779 - val_acc: 0.7472\n",
      "Epoch 28/500\n",
      "4547/4547 [==============================] - 1s 135us/sample - loss: 7.3082e-11 - acc: 1.0000 - val_loss: 6.4779 - val_acc: 0.7472\n",
      "Epoch 29/500\n",
      "4547/4547 [==============================] - 1s 121us/sample - loss: 7.3069e-11 - acc: 1.0000 - val_loss: 6.4779 - val_acc: 0.7472\n",
      "Epoch 30/500\n",
      "4547/4547 [==============================] - 1s 155us/sample - loss: 7.3046e-11 - acc: 1.0000 - val_loss: 6.4780 - val_acc: 0.7472\n",
      "Epoch 31/500\n",
      "4547/4547 [==============================] - 1s 119us/sample - loss: 7.3029e-11 - acc: 1.0000 - val_loss: 6.4780 - val_acc: 0.7472\n",
      "Epoch 32/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 7.3012e-11 - acc: 1.0000 - val_loss: 6.4780 - val_acc: 0.7472\n",
      "Epoch 33/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 7.2988e-11 - acc: 1.0000 - val_loss: 6.4781 - val_acc: 0.7472\n",
      "Epoch 34/500\n",
      "4547/4547 [==============================] - 1s 139us/sample - loss: 7.2968e-11 - acc: 1.0000 - val_loss: 6.4781 - val_acc: 0.7472\n",
      "Epoch 35/500\n",
      "4547/4547 [==============================] - 1s 157us/sample - loss: 7.2933e-11 - acc: 1.0000 - val_loss: 6.4782 - val_acc: 0.7472\n",
      "Epoch 36/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 7.2912e-11 - acc: 1.0000 - val_loss: 6.4782 - val_acc: 0.7472\n",
      "Epoch 37/500\n",
      "4547/4547 [==============================] - 1s 125us/sample - loss: 7.2883e-11 - acc: 1.0000 - val_loss: 6.4783 - val_acc: 0.7472\n",
      "Epoch 38/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 7.2851e-11 - acc: 1.0000 - val_loss: 6.4784 - val_acc: 0.7472\n",
      "Epoch 39/500\n",
      "4547/4547 [==============================] - 1s 139us/sample - loss: 7.2813e-11 - acc: 1.0000 - val_loss: 6.4785 - val_acc: 0.7472\n",
      "Epoch 40/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 7.2784e-11 - acc: 1.0000 - val_loss: 6.4785 - val_acc: 0.7472\n",
      "Epoch 41/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 7.2731e-11 - acc: 1.0000 - val_loss: 6.4786 - val_acc: 0.7472\n",
      "Epoch 42/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 7.2695e-11 - acc: 1.0000 - val_loss: 6.4787 - val_acc: 0.7472\n",
      "Epoch 43/500\n",
      "4547/4547 [==============================] - 1s 121us/sample - loss: 7.2653e-11 - acc: 1.0000 - val_loss: 6.4788 - val_acc: 0.7472\n",
      "Epoch 44/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 7.2592e-11 - acc: 1.0000 - val_loss: 6.4790 - val_acc: 0.7472\n",
      "Epoch 45/500\n",
      "4547/4547 [==============================] - 1s 118us/sample - loss: 7.2527e-11 - acc: 1.0000 - val_loss: 6.4791 - val_acc: 0.7472\n",
      "Epoch 46/500\n",
      "4547/4547 [==============================] - 1s 121us/sample - loss: 7.2492e-11 - acc: 1.0000 - val_loss: 6.4792 - val_acc: 0.7472\n",
      "Epoch 47/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 7.2413e-11 - acc: 1.0000 - val_loss: 6.4794 - val_acc: 0.7472\n",
      "Epoch 48/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 7.2342e-11 - acc: 1.0000 - val_loss: 6.4796 - val_acc: 0.7472\n",
      "Epoch 49/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 7.2281e-11 - acc: 1.0000 - val_loss: 6.4797 - val_acc: 0.7472\n",
      "Epoch 50/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 7.2192e-11 - acc: 1.0000 - val_loss: 6.4799 - val_acc: 0.7472\n",
      "Epoch 51/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 7.2099e-11 - acc: 1.0000 - val_loss: 6.4801 - val_acc: 0.7472\n",
      "Epoch 52/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 7.2006e-11 - acc: 1.0000 - val_loss: 6.4803 - val_acc: 0.7472\n",
      "Epoch 53/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 7.1902e-11 - acc: 1.0000 - val_loss: 6.4806 - val_acc: 0.7472\n",
      "Epoch 54/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 7.1820e-11 - acc: 1.0000 - val_loss: 6.4808 - val_acc: 0.7472\n",
      "Epoch 55/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 7.1692e-11 - acc: 1.0000 - val_loss: 6.4810 - val_acc: 0.7472\n",
      "Epoch 56/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 7.1586e-11 - acc: 1.0000 - val_loss: 6.4813 - val_acc: 0.7472\n",
      "Epoch 57/500\n",
      "4547/4547 [==============================] - 1s 125us/sample - loss: 7.1451e-11 - acc: 1.0000 - val_loss: 6.4817 - val_acc: 0.7472\n",
      "Epoch 58/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 7.1323e-11 - acc: 1.0000 - val_loss: 6.4820 - val_acc: 0.7472\n",
      "Epoch 59/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 7.1184e-11 - acc: 1.0000 - val_loss: 6.4823 - val_acc: 0.7472\n",
      "Epoch 60/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 7.1046e-11 - acc: 1.0000 - val_loss: 6.4827 - val_acc: 0.7472\n",
      "Epoch 61/500\n",
      "4547/4547 [==============================] - 1s 125us/sample - loss: 7.0863e-11 - acc: 1.0000 - val_loss: 6.4831 - val_acc: 0.7472\n",
      "Epoch 62/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 7.0678e-11 - acc: 1.0000 - val_loss: 6.4835 - val_acc: 0.7472\n",
      "Epoch 63/500\n",
      "4547/4547 [==============================] - 1s 135us/sample - loss: 7.0516e-11 - acc: 1.0000 - val_loss: 6.4839 - val_acc: 0.7472\n",
      "Epoch 64/500\n",
      "4547/4547 [==============================] - 1s 168us/sample - loss: 7.0354e-11 - acc: 1.0000 - val_loss: 6.4844 - val_acc: 0.7472\n",
      "Epoch 65/500\n",
      "4547/4547 [==============================] - 1s 153us/sample - loss: 7.0097e-11 - acc: 1.0000 - val_loss: 6.4851 - val_acc: 0.7472\n",
      "Epoch 66/500\n",
      "4547/4547 [==============================] - 1s 150us/sample - loss: 6.9899e-11 - acc: 1.0000 - val_loss: 6.4856 - val_acc: 0.7472\n",
      "Epoch 67/500\n",
      "4547/4547 [==============================] - 1s 174us/sample - loss: 6.9649e-11 - acc: 1.0000 - val_loss: 6.4862 - val_acc: 0.7472\n",
      "Epoch 68/500\n",
      "4547/4547 [==============================] - 1s 198us/sample - loss: 6.9389e-11 - acc: 1.0000 - val_loss: 6.4868 - val_acc: 0.7472\n",
      "Epoch 69/500\n",
      "4547/4547 [==============================] - 1s 198us/sample - loss: 6.9142e-11 - acc: 1.0000 - val_loss: 6.4875 - val_acc: 0.7472\n",
      "Epoch 70/500\n",
      "4547/4547 [==============================] - 1s 184us/sample - loss: 6.8866e-11 - acc: 1.0000 - val_loss: 6.4882 - val_acc: 0.7472\n",
      "Epoch 71/500\n",
      "4547/4547 [==============================] - 1s 142us/sample - loss: 6.8557e-11 - acc: 1.0000 - val_loss: 6.4890 - val_acc: 0.7472\n",
      "Epoch 72/500\n",
      "4547/4547 [==============================] - 1s 162us/sample - loss: 6.8254e-11 - acc: 1.0000 - val_loss: 6.4898 - val_acc: 0.7472\n",
      "Epoch 73/500\n",
      "4547/4547 [==============================] - 1s 188us/sample - loss: 6.8028e-11 - acc: 1.0000 - val_loss: 6.4904 - val_acc: 0.7472\n",
      "Epoch 74/500\n",
      "4547/4547 [==============================] - 1s 161us/sample - loss: 6.7662e-11 - acc: 1.0000 - val_loss: 6.4915 - val_acc: 0.7472\n",
      "Epoch 75/500\n",
      "4547/4547 [==============================] - 1s 173us/sample - loss: 6.7279e-11 - acc: 1.0000 - val_loss: 6.4925 - val_acc: 0.7472\n",
      "Epoch 76/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 6.6947e-11 - acc: 1.0000 - val_loss: 6.4935 - val_acc: 0.7472\n",
      "Epoch 77/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 6.6532e-11 - acc: 1.0000 - val_loss: 6.4945 - val_acc: 0.7472\n",
      "Epoch 78/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 6.6189e-11 - acc: 1.0000 - val_loss: 6.4956 - val_acc: 0.7472\n",
      "Epoch 79/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 6.5802e-11 - acc: 1.0000 - val_loss: 6.4968 - val_acc: 0.7472\n",
      "Epoch 80/500\n",
      "4547/4547 [==============================] - 1s 142us/sample - loss: 6.5342e-11 - acc: 1.0000 - val_loss: 6.4981 - val_acc: 0.7472\n",
      "Epoch 81/500\n",
      "4547/4547 [==============================] - 1s 174us/sample - loss: 6.4809e-11 - acc: 1.0000 - val_loss: 6.4991 - val_acc: 0.7472\n",
      "Epoch 82/500\n",
      "4547/4547 [==============================] - 1s 170us/sample - loss: 6.4411e-11 - acc: 1.0000 - val_loss: 6.5007 - val_acc: 0.7472\n",
      "Epoch 83/500\n",
      "4547/4547 [==============================] - 1s 155us/sample - loss: 6.3888e-11 - acc: 1.0000 - val_loss: 6.5021 - val_acc: 0.7472\n",
      "Epoch 84/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 6.3336e-11 - acc: 1.0000 - val_loss: 6.5037 - val_acc: 0.7472\n",
      "Epoch 85/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 6.2928e-11 - acc: 1.0000 - val_loss: 6.5051 - val_acc: 0.7472\n",
      "Epoch 86/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 6.2141e-11 - acc: 1.0000 - val_loss: 6.5072 - val_acc: 0.7472\n",
      "Epoch 87/500\n",
      "4547/4547 [==============================] - 1s 153us/sample - loss: 6.1657e-11 - acc: 1.0000 - val_loss: 6.5088 - val_acc: 0.7472\n",
      "Epoch 88/500\n",
      "4547/4547 [==============================] - 1s 144us/sample - loss: 6.1060e-11 - acc: 1.0000 - val_loss: 6.5105 - val_acc: 0.7472\n",
      "Epoch 89/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 6.0433e-11 - acc: 1.0000 - val_loss: 6.5128 - val_acc: 0.7472\n",
      "Epoch 90/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 5.9782e-11 - acc: 1.0000 - val_loss: 6.5149 - val_acc: 0.7472\n",
      "Epoch 91/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 5.9112e-11 - acc: 1.0000 - val_loss: 6.5170 - val_acc: 0.7472\n",
      "Epoch 92/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 5.8417e-11 - acc: 1.0000 - val_loss: 6.5193 - val_acc: 0.7472\n",
      "Epoch 93/500\n",
      "4547/4547 [==============================] - 1s 124us/sample - loss: 5.7737e-11 - acc: 1.0000 - val_loss: 6.5216 - val_acc: 0.7472\n",
      "Epoch 94/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 5.6961e-11 - acc: 1.0000 - val_loss: 6.5239 - val_acc: 0.7472\n",
      "Epoch 95/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 5.6345e-11 - acc: 1.0000 - val_loss: 6.5267 - val_acc: 0.7472\n",
      "Epoch 96/500\n",
      "4547/4547 [==============================] - 1s 163us/sample - loss: 5.5533e-11 - acc: 1.0000 - val_loss: 6.5293 - val_acc: 0.7472\n",
      "Epoch 97/500\n",
      "4547/4547 [==============================] - 1s 215us/sample - loss: 5.4690e-11 - acc: 1.0000 - val_loss: 6.5319 - val_acc: 0.7472\n",
      "Epoch 98/500\n",
      "4547/4547 [==============================] - 1s 202us/sample - loss: 5.3780e-11 - acc: 1.0000 - val_loss: 6.5351 - val_acc: 0.7472\n",
      "Epoch 99/500\n",
      "4547/4547 [==============================] - 1s 196us/sample - loss: 5.3025e-11 - acc: 1.0000 - val_loss: 6.5382 - val_acc: 0.7472\n",
      "Epoch 100/500\n",
      "4547/4547 [==============================] - 1s 182us/sample - loss: 5.2259e-11 - acc: 1.0000 - val_loss: 6.5411 - val_acc: 0.7472\n",
      "Epoch 101/500\n",
      "4547/4547 [==============================] - 1s 152us/sample - loss: 5.1319e-11 - acc: 1.0000 - val_loss: 6.5436 - val_acc: 0.7472\n",
      "Epoch 102/500\n",
      "4547/4547 [==============================] - 1s 139us/sample - loss: 5.0888e-11 - acc: 1.0000 - val_loss: 6.5458 - val_acc: 0.7472\n",
      "Epoch 103/500\n",
      "4547/4547 [==============================] - 1s 168us/sample - loss: 5.0399e-11 - acc: 1.0000 - val_loss: 6.5476 - val_acc: 0.7472\n",
      "Epoch 104/500\n",
      "4547/4547 [==============================] - 1s 172us/sample - loss: 4.9707e-11 - acc: 1.0000 - val_loss: 6.5506 - val_acc: 0.7472\n",
      "Epoch 105/500\n",
      "4547/4547 [==============================] - 1s 137us/sample - loss: 4.9101e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 106/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 107/500\n",
      "4547/4547 [==============================] - 1s 135us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 108/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 109/500\n",
      "4547/4547 [==============================] - 1s 137us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 110/500\n",
      "4547/4547 [==============================] - 1s 159us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 111/500\n",
      "4547/4547 [==============================] - 1s 186us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 112/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 113/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 114/500\n",
      "4547/4547 [==============================] - 0s 107us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 115/500\n",
      "4547/4547 [==============================] - 0s 107us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 116/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 117/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 118/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 119/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 120/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 121/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 122/500\n",
      "4547/4547 [==============================] - 1s 117us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 123/500\n",
      "4547/4547 [==============================] - 1s 120us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 124/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 125/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 126/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 127/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 128/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 129/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 130/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 131/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 132/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 133/500\n",
      "4547/4547 [==============================] - 1s 112us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 134/500\n",
      "4547/4547 [==============================] - 1s 111us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 135/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 136/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 137/500\n",
      "4547/4547 [==============================] - 0s 107us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 138/500\n",
      "4547/4547 [==============================] - 1s 122us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 139/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 140/500\n",
      "4547/4547 [==============================] - 1s 111us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 141/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 142/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 143/500\n",
      "4547/4547 [==============================] - 1s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 144/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 145/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 146/500\n",
      "4547/4547 [==============================] - 1s 116us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 147/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 148/500\n",
      "4547/4547 [==============================] - 1s 111us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 149/500\n",
      "4547/4547 [==============================] - 1s 112us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 150/500\n",
      "4547/4547 [==============================] - 1s 114us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 151/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 152/500\n",
      "4547/4547 [==============================] - 1s 120us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 153/500\n",
      "4547/4547 [==============================] - 1s 124us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 154/500\n",
      "4547/4547 [==============================] - 1s 119us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 155/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 156/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 157/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 158/500\n",
      "4547/4547 [==============================] - 1s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 159/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 160/500\n",
      "4547/4547 [==============================] - 1s 114us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 161/500\n",
      "4547/4547 [==============================] - 1s 136us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 162/500\n",
      "4547/4547 [==============================] - 1s 148us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 163/500\n",
      "4547/4547 [==============================] - 1s 167us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 164/500\n",
      "4547/4547 [==============================] - 1s 156us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 165/500\n",
      "4547/4547 [==============================] - 1s 170us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 166/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 167/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 168/500\n",
      "4547/4547 [==============================] - 1s 149us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 169/500\n",
      "4547/4547 [==============================] - 1s 152us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 170/500\n",
      "4547/4547 [==============================] - 1s 143us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 171/500\n",
      "4547/4547 [==============================] - 1s 120us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 172/500\n",
      "4547/4547 [==============================] - 1s 120us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 173/500\n",
      "4547/4547 [==============================] - 1s 124us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 174/500\n",
      "4547/4547 [==============================] - 1s 117us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 175/500\n",
      "4547/4547 [==============================] - 1s 118us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 176/500\n",
      "4547/4547 [==============================] - 1s 112us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 177/500\n",
      "4547/4547 [==============================] - 0s 102us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 178/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 179/500\n",
      "4547/4547 [==============================] - 1s 119us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 180/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 181/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 182/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 183/500\n",
      "4547/4547 [==============================] - 1s 120us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 184/500\n",
      "4547/4547 [==============================] - 1s 145us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 185/500\n",
      "4547/4547 [==============================] - 1s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 186/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 187/500\n",
      "4547/4547 [==============================] - 0s 101us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 188/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 189/500\n",
      "4547/4547 [==============================] - 1s 112us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 190/500\n",
      "4547/4547 [==============================] - 1s 120us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 191/500\n",
      "4547/4547 [==============================] - 1s 115us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 192/500\n",
      "4547/4547 [==============================] - 1s 115us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 193/500\n",
      "4547/4547 [==============================] - 1s 116us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 194/500\n",
      "4547/4547 [==============================] - 0s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 195/500\n",
      "4547/4547 [==============================] - 1s 120us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 196/500\n",
      "4547/4547 [==============================] - 0s 107us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 197/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 198/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 199/500\n",
      "4547/4547 [==============================] - 1s 150us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 200/500\n",
      "4547/4547 [==============================] - 1s 144us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 201/500\n",
      "4547/4547 [==============================] - 1s 141us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 202/500\n",
      "4547/4547 [==============================] - 1s 120us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 203/500\n",
      "4547/4547 [==============================] - 1s 144us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 204/500\n",
      "4547/4547 [==============================] - 1s 139us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 205/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 206/500\n",
      "4547/4547 [==============================] - 1s 112us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 207/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 208/500\n",
      "4547/4547 [==============================] - 1s 125us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 209/500\n",
      "4547/4547 [==============================] - 1s 135us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 210/500\n",
      "4547/4547 [==============================] - 1s 145us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 211/500\n",
      "4547/4547 [==============================] - 1s 144us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 212/500\n",
      "4547/4547 [==============================] - 1s 118us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 213/500\n",
      "4547/4547 [==============================] - 1s 119us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 214/500\n",
      "4547/4547 [==============================] - 1s 122us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 215/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 216/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 217/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 218/500\n",
      "4547/4547 [==============================] - 0s 102us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 219/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 220/500\n",
      "4547/4547 [==============================] - 0s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 221/500\n",
      "4547/4547 [==============================] - 1s 135us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 222/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 223/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 224/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 225/500\n",
      "4547/4547 [==============================] - 0s 102us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 226/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 227/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 228/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 229/500\n",
      "4547/4547 [==============================] - 0s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 230/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 231/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 232/500\n",
      "4547/4547 [==============================] - 1s 111us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 233/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 234/500\n",
      "4547/4547 [==============================] - 1s 111us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 235/500\n",
      "4547/4547 [==============================] - 1s 112us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 236/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 237/500\n",
      "4547/4547 [==============================] - 1s 159us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 238/500\n",
      "4547/4547 [==============================] - 1s 182us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 239/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 240/500\n",
      "4547/4547 [==============================] - 1s 125us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 241/500\n",
      "4547/4547 [==============================] - 1s 118us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 242/500\n",
      "4547/4547 [==============================] - 1s 111us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 243/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 244/500\n",
      "4547/4547 [==============================] - 0s 102us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 245/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 246/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 247/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 248/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 249/500\n",
      "4547/4547 [==============================] - 1s 115us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 250/500\n",
      "4547/4547 [==============================] - 1s 115us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 251/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 252/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 253/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 254/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 255/500\n",
      "4547/4547 [==============================] - 0s 102us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 256/500\n",
      "4547/4547 [==============================] - 1s 116us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 257/500\n",
      "4547/4547 [==============================] - 0s 107us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 258/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 259/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 260/500\n",
      "4547/4547 [==============================] - 1s 111us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 261/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 262/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 263/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 264/500\n",
      "4547/4547 [==============================] - 0s 107us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 265/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 266/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 267/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 268/500\n",
      "4547/4547 [==============================] - 0s 107us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 269/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 270/500\n",
      "4547/4547 [==============================] - 1s 115us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 271/500\n",
      "4547/4547 [==============================] - 1s 112us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 272/500\n",
      "4547/4547 [==============================] - 1s 146us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 273/500\n",
      "4547/4547 [==============================] - 1s 161us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 274/500\n",
      "4547/4547 [==============================] - 1s 167us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 275/500\n",
      "4547/4547 [==============================] - 1s 125us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 276/500\n",
      "4547/4547 [==============================] - 1s 144us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 277/500\n",
      "4547/4547 [==============================] - 1s 114us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 278/500\n",
      "4547/4547 [==============================] - 1s 113us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 279/500\n",
      "4547/4547 [==============================] - 1s 114us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 280/500\n",
      "4547/4547 [==============================] - 0s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 281/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 282/500\n",
      "4547/4547 [==============================] - 1s 117us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 283/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 284/500\n",
      "4547/4547 [==============================] - 1s 117us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 285/500\n",
      "4547/4547 [==============================] - 1s 124us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 286/500\n",
      "4547/4547 [==============================] - 1s 116us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 287/500\n",
      "4547/4547 [==============================] - 1s 115us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 288/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 289/500\n",
      "4547/4547 [==============================] - 0s 104us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 290/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 291/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 292/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 293/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 294/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 295/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 296/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 297/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 298/500\n",
      "4547/4547 [==============================] - 1s 114us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 299/500\n",
      "4547/4547 [==============================] - 1s 120us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 300/500\n",
      "4547/4547 [==============================] - 1s 112us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 301/500\n",
      "4547/4547 [==============================] - 1s 118us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 302/500\n",
      "4547/4547 [==============================] - 1s 125us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 303/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 304/500\n",
      "4547/4547 [==============================] - 0s 102us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 305/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 306/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 307/500\n",
      "4547/4547 [==============================] - 0s 102us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 308/500\n",
      "4547/4547 [==============================] - 0s 103us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 309/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 310/500\n",
      "4547/4547 [==============================] - 1s 111us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 311/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 312/500\n",
      "4547/4547 [==============================] - 1s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 313/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 314/500\n",
      "4547/4547 [==============================] - 0s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 315/500\n",
      "4547/4547 [==============================] - 0s 107us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 316/500\n",
      "4547/4547 [==============================] - 1s 110us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 317/500\n",
      "4547/4547 [==============================] - 1s 123us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 318/500\n",
      "4547/4547 [==============================] - 1s 114us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 319/500\n",
      "4547/4547 [==============================] - 1s 111us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 320/500\n",
      "4547/4547 [==============================] - 1s 121us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 321/500\n",
      "4547/4547 [==============================] - 1s 123us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 322/500\n",
      "4547/4547 [==============================] - 1s 124us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 323/500\n",
      "4547/4547 [==============================] - 1s 123us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 324/500\n",
      "4547/4547 [==============================] - 1s 114us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 325/500\n",
      "4547/4547 [==============================] - 0s 109us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 326/500\n",
      "4547/4547 [==============================] - 0s 107us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 327/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 328/500\n",
      "4547/4547 [==============================] - 0s 106us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 329/500\n",
      "4547/4547 [==============================] - 0s 105us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 330/500\n",
      "4547/4547 [==============================] - 0s 108us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 331/500\n",
      "4547/4547 [==============================] - 1s 115us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 332/500\n",
      "4547/4547 [==============================] - 1s 118us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 333/500\n",
      "4547/4547 [==============================] - 1s 122us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 334/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 335/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 336/500\n",
      "4547/4547 [==============================] - 1s 179us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 337/500\n",
      "4547/4547 [==============================] - 1s 174us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 338/500\n",
      "4547/4547 [==============================] - 1s 182us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 339/500\n",
      "4547/4547 [==============================] - 1s 193us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 340/500\n",
      "4547/4547 [==============================] - 1s 152us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 341/500\n",
      "4547/4547 [==============================] - 1s 150us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 342/500\n",
      "4547/4547 [==============================] - 1s 150us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 343/500\n",
      "4547/4547 [==============================] - 1s 145us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 344/500\n",
      "4547/4547 [==============================] - 1s 149us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 345/500\n",
      "4547/4547 [==============================] - 1s 151us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 346/500\n",
      "4547/4547 [==============================] - 1s 147us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 347/500\n",
      "4547/4547 [==============================] - 1s 149us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 348/500\n",
      "4547/4547 [==============================] - 1s 148us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 349/500\n",
      "4547/4547 [==============================] - 1s 154us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 350/500\n",
      "4547/4547 [==============================] - 1s 150us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 351/500\n",
      "4547/4547 [==============================] - 1s 147us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 352/500\n",
      "4547/4547 [==============================] - 1s 147us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 353/500\n",
      "4547/4547 [==============================] - 1s 137us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 354/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 355/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 356/500\n",
      "4547/4547 [==============================] - 1s 147us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 357/500\n",
      "4547/4547 [==============================] - 1s 152us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 358/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 359/500\n",
      "4547/4547 [==============================] - 1s 143us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 360/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 361/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 362/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 363/500\n",
      "4547/4547 [==============================] - 1s 135us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 364/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 365/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 366/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 367/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 368/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 369/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 370/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 371/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 372/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 373/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 374/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 375/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 376/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 377/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 378/500\n",
      "4547/4547 [==============================] - 1s 139us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 379/500\n",
      "4547/4547 [==============================] - 1s 145us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 380/500\n",
      "4547/4547 [==============================] - 1s 150us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 381/500\n",
      "4547/4547 [==============================] - 1s 147us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 382/500\n",
      "4547/4547 [==============================] - 1s 156us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 383/500\n",
      "4547/4547 [==============================] - 1s 162us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 384/500\n",
      "4547/4547 [==============================] - 1s 146us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 385/500\n",
      "4547/4547 [==============================] - 1s 142us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 386/500\n",
      "4547/4547 [==============================] - 1s 138us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 387/500\n",
      "4547/4547 [==============================] - 1s 136us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 388/500\n",
      "4547/4547 [==============================] - 1s 145us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 389/500\n",
      "4547/4547 [==============================] - 1s 146us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 390/500\n",
      "4547/4547 [==============================] - 1s 135us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 391/500\n",
      "4547/4547 [==============================] - 1s 145us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 392/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 393/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 394/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 395/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 396/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 397/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 398/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 399/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 400/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 401/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 402/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 403/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 404/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 405/500\n",
      "4547/4547 [==============================] - 1s 136us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 406/500\n",
      "4547/4547 [==============================] - 1s 142us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 407/500\n",
      "4547/4547 [==============================] - 1s 144us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 408/500\n",
      "4547/4547 [==============================] - 1s 146us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 409/500\n",
      "4547/4547 [==============================] - 1s 153us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 410/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 411/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 412/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 413/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 414/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 415/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 416/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 417/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 418/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 419/500\n",
      "4547/4547 [==============================] - 1s 135us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 420/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 421/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 422/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 423/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 424/500\n",
      "4547/4547 [==============================] - 1s 142us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 425/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 426/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 427/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 428/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 429/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 430/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 431/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 432/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 433/500\n",
      "4547/4547 [==============================] - 1s 151us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 434/500\n",
      "4547/4547 [==============================] - 1s 154us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 435/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 436/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 437/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 438/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 439/500\n",
      "4547/4547 [==============================] - 1s 135us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 440/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 441/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 442/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 443/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 444/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 445/500\n",
      "4547/4547 [==============================] - 1s 138us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 446/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 447/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 448/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 449/500\n",
      "4547/4547 [==============================] - 1s 126us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 450/500\n",
      "4547/4547 [==============================] - 1s 143us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 451/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 452/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 453/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 454/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 455/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 456/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 457/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 458/500\n",
      "4547/4547 [==============================] - 1s 145us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 459/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 460/500\n",
      "4547/4547 [==============================] - 1s 159us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 461/500\n",
      "4547/4547 [==============================] - 1s 143us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 462/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 463/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 464/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 465/500\n",
      "4547/4547 [==============================] - 1s 137us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 466/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 467/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 468/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 469/500\n",
      "4547/4547 [==============================] - 1s 139us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 470/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 471/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 472/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 473/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 474/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 475/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 476/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 477/500\n",
      "4547/4547 [==============================] - 1s 136us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 478/500\n",
      "4547/4547 [==============================] - 1s 134us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 479/500\n",
      "4547/4547 [==============================] - 1s 138us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 480/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 481/500\n",
      "4547/4547 [==============================] - 1s 128us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 482/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 483/500\n",
      "4547/4547 [==============================] - 1s 141us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 484/500\n",
      "4547/4547 [==============================] - 1s 143us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 485/500\n",
      "4547/4547 [==============================] - 1s 138us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 486/500\n",
      "4547/4547 [==============================] - 1s 145us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 487/500\n",
      "4547/4547 [==============================] - 1s 145us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 488/500\n",
      "4547/4547 [==============================] - 1s 140us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 489/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 490/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 491/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 492/500\n",
      "4547/4547 [==============================] - 1s 136us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 493/500\n",
      "4547/4547 [==============================] - 1s 129us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 494/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 495/500\n",
      "4547/4547 [==============================] - 1s 130us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 496/500\n",
      "4547/4547 [==============================] - 1s 133us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 497/500\n",
      "4547/4547 [==============================] - 1s 138us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 498/500\n",
      "4547/4547 [==============================] - 1s 127us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 499/500\n",
      "4547/4547 [==============================] - 1s 132us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n",
      "Epoch 500/500\n",
      "4547/4547 [==============================] - 1s 131us/sample - loss: 4.8977e-11 - acc: 1.0000 - val_loss: 6.5520 - val_acc: 0.7472\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = model.fit((np.array(Xnf_train), np.array(y_train)), epochs=500, validation_data=(np.array(Xnf_test), np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "[0 0 1 ... 1 1 0]\n",
      "[[   0  493]\n",
      " [   0 1457]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7471794871794872"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = model.predict_classes(Xnf_test)\n",
    "y_test = np.array(y_test)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "y_test = y_test.reshape((np.array(y_test)).shape[0], 1)\n",
    "cm = confusion_matrix(np.array(y_test), y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(np.array(Xn_test), np.array(y_test), verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "de28e1e25f48b68b9638a29c8142fb7816a4ec90feebba1cd15c63ff7df4d74f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
